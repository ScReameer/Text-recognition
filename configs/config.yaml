LABELS: '!%&,-./0123456789=_abcdefghijklmnopqrstuvwxyz' # Vocabulary
CASE_SENSITIVE: False # True if you want to treat labels as case sensitive
ClearML:
  ENABLED: False # True if you have configured clearml.conf, else False
  PROJECT: 'ViTSTR-T' # ClearML project name
  TASK: 'text-recognition' # ClearML task name
DATASET:
  TYPE: 'lmdb' # "lmdb" or "json"
  # Path to dataset with 'train', 'val' and 'test' folders
  # NOTE: if you use docker container you should specify your local dataset path in '.env' file
  PATH: '/dataset' 
ViTSTR-T:
  # Pretrained weights for training, should be 'vit' or 'vitstr_tiny' or 'vitstr_small' or 'vitstr_base'
  # vit: 6.7M params / 26.7 MB
  # vitstr_tiny: 6.7M params / 26.7 MB
  # vitstr_small: 24.6M params / 98.5 MB
  # vitstr_base: 94.4M params / 377.4 MB
  BACKBONE_TYPE: 'vitstr_base' 
  IMG_SIZE: [112, 448] # Default is [224, 224], but [112, 448] can be better for long words
  INPUT_CHANNELS: 3 # 1 (monochrome) or 3 (RGB)
  NUM_WORKERS: 0 # 0 - auto detect
  TRAIN:
    BATCH_SIZE: 64
    FOLDERS: ['train', 'val'] # Two folder names with train and eval data
    MAX_EPOCHS: 1 # 1 is enough if you have big train dataset (>5m images)
    LOSS: 'focal_loss' # "focal_loss" or "cross_entropy". In practice, Focal Loss converges faster for this task
    LR: 5e-5 # Initial learning rate
    WEIGHT_DECAY: 1e-3 # decoupled weight decay
    DROPOUT_RATE: 0.1
    GAMMA: 0.98 # Exponential multiplier for learning rate per epoch
    PATIENCE: 50 # EarlyStopping 'patience' parameter
    MIN_DELTA: 1e-3 # EarlyStopping 'min_delta' parameter
    # Attention backend for training (don't affect inference after loading *.ckpt or *.torchscript model), default is "FLASH_ATTENTION".
    # You can choose one of the following: ["MATH", "FLASH_ATTENTION", "EFFICIENT_ATTENTION", "CUDNN_ATTENTION"].
    # If you want to add this during the inference, follow these instructions (you can use this context manager for entire forward method):
    # https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html
    SDPBackend: "FLASH_ATTENTION"
    # one of ["64", "64-true", "32", "32-true", "16", "16-mixed", "bf16", "bf16-mixed"]
    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/trainer/trainer.py
    PRECISION: "bf16-mixed" 
  TEST:
    BATCH_SIZE: 1 # Batch size for testing. Leave it 1 to test average inference performance per image, otherwise you can set a larger batch size.
    FOLDER: 'test' # One folder name with test data
